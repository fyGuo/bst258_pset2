---
title: "Problem Set #1"
subtitle: "BST 258: Causal Inference -- Theory and Practice"
author: "Fuyu Guo"
date: "2023-02-15"
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
    fontsize: 11pt
    geometry:
      - margin=1in
      - heightrounded
    number-sections: false
    colorlinks: true
    link-citations: true
    callout-appearance: simple
    callout-icon: false
    # figure options
    fig-width: 6
    fig-asp: 0.618
    fig-cap-location: bottom
    # code block options
    code-line-numbers: false
    code-block-bg: false
    highlight-style: nord
bibliography: refs.bib
---

```{r}
#| echo: false
#| message: false
#| label: global-setup
# NOTE: The immediately following line loads an renv environment located at the
#       nearest "top-level" directory, as marked by a `.here` file, which is
#       located by the here::here() function. This would be a useful tool if,
#       say, this template.qmd file was not located at the top-level directory.
#       Here, renv should activate automatically when this file is opened.
#renv::load(here::here())
library(here)
```

\footnotesize
**Problem set policies:** Please provide concise, clear answers for each
question. Note that only writing the result of a calculation without
explanation is not sufficient. For problems involving \textsf{R}, include the
code in your solution, along with any figures or tables.

Please submit your problem set via Canvas in PDF, alongside the corresponding
\textsf{Quarto} (`.qmd`) source file.

We encourage you to discuss problems with other students, but you must
write your final answer in your own words. Solutions prepared "in
committee" are not acceptable. If you do collaborate with classmates on a
problem, please list your collaborators on your solution. Similarly, while you
may find the use of ChatGPT and related tools helpful, disclosing your use of
them is required to adhere to academic/intellectual honesty.

::: {.callout-warning title="Coding Standards"}
Make sure to write code that is modular and reusable, making use of functions
or classes as necessary. Make sure to use variable names that are descriptive
--- that is, human parse-able (e.g., not `x` or `this_var`) --- while favoring
conciseness. Write brief documentation for any functions or classes as well as
any unit tests necessary you deem necessary to ensure that your code is working
as expected. Finally, make to set a seed for the PRNG so that any random aspects
of your analysis (e.g., Monte Carlo simulation experiments, sample-splitting)
are reproducible.
:::

Remember that \textsf{Quarto} extends markdown, which supports \LaTeX for
typesetting equations, and also allows for code and analytic results to be
interspersed in a single document, consistent with literate programming
principles.

Put your answer in ["callout"
blocks](https://quarto.org/docs/authoring/callouts.html) like the following

:::{.callout-note title="Answer"}
Here is my answer. The logic follows, but first I will typeset a definition
to clarify my point: $P_n f(X) = \frac{1}{n} f(X_i)$.

I can also create "displayed equations," using, e.g., the `align` environment,
\begin{align*}
  L &\sim \text{Unif}(-1, 1) \\
  A &\sim \text{Bern}(0.5) \\
  Y &\sim \text{N}(\mu = 2 A + 0.5 L, \sigma = 1)
\end{align*}
:::

You can also write and run \textsf{R} code. As an example, below, we load two
packages, `data.table` and `tidyverse`, and set a seed for the PRNG.

```{r}
#| label: example-code
library(data.table)
library(tidyverse)
set.seed(11249)
```

You may also cite sources from the `refs.bib` file, e.g., @hernan2023causal.
\normalsize

{{< pagebreak >}}



## Question 1

\textcolor{blue}{I forked the repo and cloned it on my computer.}


{{< pagebreak >}}


## Question 2

### a) 

\textcolor{blue}{The marginal distribution of the treatment $A$ is} 

\textcolor{blue}{$$A \sim Bern(\frac{m}{n})$$}

### b)

\textcolor{blue}{
$$
\Pr[A_i = 1, A_j = 1] = \frac{{n-2 \choose m-2}}{{n \choose m}} = \frac{(n-2)!m!(n-m)!}{(m-2)!(n-m)! n!} = \frac{m(m-1)}{n(n-1)}$$
}


\textcolor{blue}{
$$\Pr[A_i = 0, A_j = 0] = \frac{{{n-2} \choose {m}}}{{n \choose m}} = \frac{(n-2)!m!(n-m)!}{m!(n-m-2)! n!} = \frac{(n-m)(n-m-1)}{n(n-1)}$$
}

\textcolor{blue}{
$$\Pr[A_i = 0, A_j = 1] = \Pr[A_i = 1, A_j = 0] = \frac{1- \Pr[A_i = 1, A_j = 1]  - \Pr[A_i = 0, A_j = 0]}{2} = \frac{m(n-m)}{n(n-1)}$$
}


|\textcolor{blue}{$A_i$}| \textcolor{blue}{0} |\textcolor{blue}{1} |
|---|---|---|
|\textcolor{blue}{$A_j$}|   |   |
|\textcolor{blue}{0}|\textcolor{blue}{$\frac{(n-m)(n-m-1)}{n(n-1)}$}|\textcolor{blue}{$\frac{m(n-m)}{n(n-1)}$}| 
|\textcolor{blue}{1}|\textcolor{blue}{$\frac{m(n-m)}{n(n-1)}$}|\textcolor{blue}{$\frac{m(m-1)}{n(n-1)}$}| 

{{< pagebreak >}}

### c)

\textcolor{blue}{The marginal distribution of $A$ is Bernoulli with $p = \frac{m}{n}$. Thus,} 

\textcolor{blue}{$$\mathbb{V}(A_i) = \frac{m}{n}(1-\frac{m}{n}) = \frac{m(n-m)}{n^2}$$
}


\textcolor{blue}{$$\text{Cov}(A_i, A_j) = \mathbb{E}(A_i A_j) - \mathbb{E}(A_i) \mathbb{E}(A_j)$$ for $i\ne j$}

\textcolor{blue}{$$
\begin{aligned}
\mathbb{E}(A_i A_j) &=  \Pr[A_i = 1, A_j = 1] = \frac{m(m-1)}{n(n-1)}\\
\end{aligned}
$$}
\textcolor{blue}{$$\text{Cov}(A_i, A_j) =\frac{m(m-1)}{n(n-1)} - \frac{m^2}{n^2}$$}

### d)

\textcolor{blue}{
$$
\begin{aligned}
\mathbb{E}[\theta^{\mathrm{ATT}}] &= \mathbb{E}[\frac{1}{m}\mathbb{E}(\sum_{i=1}^n A_i(Y_i(1)-Y_i(0))|\mathbf{A})]\\
&=\mathbb{E}[\frac{1}{m}\sum_{i=1}^n A_i\mathbb{E}(Y_i(1)-Y_i(0)|\mathbf{A})]\\
\end{aligned}
$$
}


\textcolor{blue}{If we think of $Y_i(1)$ and $Y_i(0)$ as fixed in the $n$ units, then $\mathbb{E}(Y_i(1)-Y_i(0)|\mathbf{A}) = $.}


\textcolor{blue}{
$$
\begin{aligned}
\mathbb{E}[\theta^{\mathrm{ATT}}] 
&=\mathbb{E}[\frac{1}{m}\sum_{i=1}^n A_i(Y_i(1)-Y_i(0))]\\
&=\frac{1}{m}\mathbb{E}[\sum_{i=1}^n A_i(Y_i(1)-Y_i(0))]\\
&=\frac{1}{m}\sum_{i=1}^n(Y_i(1)-Y_i(0))\mathbb{E}[ A_i]\\
&=\frac{1}{m}\sum_{i=1}^n(Y_i(1)-Y_i(0))\frac{m}{n}\\
&=\frac{1}{n}\sum_{i=1}^n Y_i(1)-Y_i(0)
\end{aligned}
$$
}

\textcolor{blue}{The expectation of $\theta^{\mathrm{ATT}}$ is The sample average treatment effect over the $n$ units.}

{{< pagebreak >}}

## Question 3

\textcolor{blue}{
$$
\begin{aligned}
& \operatorname{Var}\left(Y_i(1)\right) \\
= & \mathbb{E}\left(Y_i^2(1)\right)-\mathbb{E}^2\left[Y_i(1)\right] \\
= & \frac{1}{n} \sum_{i=1}^n Y_i^2(1)-\left(\frac{1}{n} \sum_{i=1}^n Y_i(1)\right)^2 \\
= & \frac{1}{n} \sum_{i=1}^n\left(Y_i(0)+\theta\right)^2-\left[\frac{1}{n} \sum_{i=1}^n\left(Y_i(0)\right)+\theta\right]^2 \\
= & \frac{1}{n}\left(\sum_{i=1}^n Y_i^2(0)+2 \sum_{i=1}^n Y_i(0)+n \theta^2\right) \\
- & \frac{1}{n^2}\left[\sum_{i=1}^n Y_i(0)\right]^2-\theta^2-\frac{2}{n}\left(\sum_{i=1}^n Y_i(0)\right) \theta \\
= & \frac{1}{n} \sum_{i=1}^n Y_i^2(0)-\frac{1}{n^2}\left[\sum_{i=1}^n Y_i(0)\right]^2 \\
= & \mathbb{E}\left[Y_i^2(0)\right]-\mathbb{E}^2\left[Y_i(0)\right] \\
= & \operatorname{Var}\left(Y_i(0)\right)
\end{aligned}
$$
}

\textcolor{blue}{
$$
\begin{aligned}
\rho\left(Y_i(1), Y_i(0)\right) & =\rho\left(Y_i(0)+\theta, Y_i(0)\right) \\
& =\rho\left(Y_i(0), Y_i(0))+\rho\left(\theta, Y_i(0)\right)\right. \\
& =1
\end{aligned}
$$
}

{{< pagebreak >}}

## Question 4

\textcolor{blue}{Denote the number of correct guesses as $X$.}


\textcolor{blue}{$$\Pr[X=0] = \frac{1}{{8 \choose 4}} =  0.014$$}
\textcolor{blue}{$$\Pr[X=1] = \frac{{4 \choose 1}{4 \choose 3}}{{8 \choose 4}} = 0.228$$}
\textcolor{blue}{$$\Pr[X=2] = \frac{{4 \choose 2}{4 \choose 2}}{{8 \choose 4}} = 0.514$$}
\textcolor{blue}{$$\Pr[X=3] = \frac{{4 \choose 3}{4 \choose 1}}{{8 \choose 4}} = 0.228$$}
\textcolor{blue}{$$\Pr[X=4] = \frac{1}{{8 \choose 4}} =  0.014$$}

{{< pagebreak >}}

## Question 5

### a)

\textcolor{blue}{It is an example of the famous Simpson' Paradox. One explanation is the possible confounding by stone sizes. From Table 1, it seems that the treatment was not randomized and treatment A was more likely to be assigned to people with large stones and treatment B was more likely to be assigned to people with small stones. That is treatment are unbalanced for groups with small stones and large stones. In addition, people with large stones has lower success rates both for treatment A and B. Thus, stone size is a confounder. Then the overall effects may not reflect the true effects of the treatment and can have discrepancy from the stratified results.} 

\textcolor{blue}{Besides the confounding by stone sizes, there might be other unmeasured confounders for treatment that we did not take into account.}


### b)

\begin{table}[ht]
\begin{tabular}{rcc}
\multicolumn{1}{l}{}             & \textcolor{blue}{Treatment A} & \textcolor{blue}{Treatment B}  \\
\multicolumn{1}{l}{\textcolor{blue}{Small Stones}} & \textcolor{blue}{81/87}       & \textcolor{blue}{234/270}     \\
\textcolor{blue}{Male}                            & \textcolor{blue}{40/43}       & \textcolor{blue}{117/135}     \\
\textcolor{blue}{Female}                          & \textcolor{blue}{41/44}       & \textcolor{blue}{117/135}     \\
\multicolumn{1}{l}{\textcolor{blue}{Large Stones}} & \textcolor{blue}{192/263}     & \textcolor{blue}{55/80}       \\
\textcolor{blue}{Male}                            & \textcolor{blue}{96/131}      & \textcolor{blue}{27/40}       \\
\textcolor{blue}{Female}                           & \textcolor{blue}{96/132}      & \textcolor{blue}{28/40}      
\end{tabular}
\end{table}

\textcolor{blue}{Among males, the success rate of treatment A is $\frac{40+96}{43+131} = 0.78$, and the success rate of treatment B is $\frac{117+27}{135+40} = 0.82$.}


\textcolor{blue}{Among females, the success rate of treatment A is $\frac{41+96}{44+131} = 0.78$, and the success rate of treatment B is $\frac{117+28}{135+40} = 0.83$.}



\textcolor{blue}{Thus, across all genders, treatment B is more effective than treatment A.}


### c)

- \textcolor{Blue}{The phenomenon is called Simpsons'Paradox. The unbalanced distribution of treatment across risk factors is called confounding.}
- \textcolor{Blue}{It reminds us that in an observational study without randomizations, we need to look at results stratified by potential confounders. The overall comparison of treatment effects may not reflect the true effects.}

{{< pagebreak >}}


## Question 6

```{r, cache=TRUE}

# write a function to generate potential outcomes
generate_potential_Y <- function(n){
  Y1 <- rnorm(n, mean = 1/10, sd = 1/4)
  Y0 <- rnorm(n, mean = 0, sd = 1/4)
  ATE <- Y1 - Y0
  return(data.frame(Y1, Y0, ATE))
}

# write a function to generate assignment
assign_treatment <- function(n) {
  # generate a data frame contains potential outcomes and treatment assignment
  df <- generate_potential_Y(n)
  index <- sample(1:n, n/2, replace = F)
  df$A <-0
  df$A[index] <- 1

  # generate the observed Y
  df$Y <- df$Y1 * df$A + df$Y0 * (1 - df$A)
  return(df)
}

# use t-test to test the weak null hypothesis
test_weak_null <- function(df) {
  results <- t.test(df$Y[df$A == 1], df$Y[df$A == 0], var.equal = TRUE)
  return(results$p.value < 0.05)
}

# use imputations to test the sharp null hypothesis
test_sharp_null <- function(df) {

    statistic <- mean(df$Y[df$A == 1]) - mean(df$Y[df$A == 0])
    
    # record the number of assignment for A = 1
    num_A1 <- sum(df$A)
    
    new_statistic <- numeric(10000)
    for (b in 1:10000) {
      ind <- sample(1:dim(df)[1], num_A1, replace = F)
      new_statistic[b] <- mean(df$Y[ind]) - mean(df$Y[-ind])
    }
    
    return(statistic < quantile(new_statistic, 0.025) | statistic > quantile(new_statistic, 0.975))
}

# for fixed n, write a function to repeat the test for nsim = 1000 trials
# use parallel to make the algorithm faster
library(future)
library(furrr)
plan(multisession)
repeat_simulation <- function(n){
    df <- assign_treatment(n)
    p_weak_null <- test_weak_null(df)
    p_sharp_null <- test_sharp_null(df)
    return(data.frame(p_weak_null, p_sharp_null))
    }

# results for 20 samples
set.seed(321)
furrr_options(seed = TRUE)
res_20 <- future_map_dfr(1:1000, ~repeat_simulation(20),
                         .options = furrr_options(seed = TRUE))

res_50 <- future_map_dfr(1:1000, ~repeat_simulation(50),
                         .options = furrr_options(seed = TRUE))

res_100 <- future_map_dfr(1:1000, ~repeat_simulation(100),
                          .options = furrr_options(seed = TRUE))

res_200 <- future_map_dfr(1:1000, ~repeat_simulation(200),
                          .options = furrr_options(seed = TRUE))

res_500 <- future_map_dfr(1:1000, ~repeat_simulation(500),
                          .options = furrr_options(seed = TRUE))

# combine the results
tmp <- data.frame(size = c(20, 50, 100, 200, 500),
                  power_weak_null = c(mean(res_20$p_weak_null), mean(res_50$p_weak_null), mean(res_100$p_weak_null), mean(res_200$p_weak_null), mean(res_500$p_weak_null)),
                  power_sharp_null = c(mean(res_20$p_sharp_null), mean(res_50$p_sharp_null), mean(res_100$p_sharp_null), mean(res_200$p_sharp_null), mean(res_500$p_sharp_null)))

library(tidyverse)
tmp <- tmp %>% pivot_longer(cols = -size, names_to = "test", values_to = "power")

ggplot(tmp) + 
  geom_line(aes(x = size, y = power, color = test)) +
  geom_point(aes(x = size, y = power, color = test, shape = test))

```
- \textcolor{blue}{ The power against the two hypotheses increase with more sample size. At each sample size, the powers are similar, qne is within the expectation. The sharp null hypothesis is much stronger than th weak null hypothesis and any data rejecting the sharp null hypothesis will also reject the weak null hypothesis. However, in the the real simulation settings, the power again the weak null assumption can be a bit higher than the power against the sharp null hypothesis due to random error in the algorithm for the sharp null hypothesis.}
 \textcolor{blue}{ In this simulation, everyone shares the same distribution for potential outcomes. Thus, the individual causal effects are equal to the average causal effect. Therefore, the power against the sharp null hypothesis is similar to the power against the weak null hypothesis.}

{{< pagebreak >}}


## Question 7

### a)

\textcolor{blue}{
$$
\begin{array}{ll}
\min _{\alpha \cdot \beta} & \frac{1}{2 n} \sum_{i=1}^n\left(Y_i-\alpha-\beta A_i\right)^2 \\
& \frac{\partial}{\partial \alpha} \frac{1}{2 n} \sum_{i=1}^n\left(Y_i-\alpha-\beta A_i\right)^2=0 \\
\Rightarrow & \sum_{i=1}^n\left(Y_i-\alpha-\beta A_i\right)=0 \\
\Rightarrow & \sum_{i=1}^n Y_i-n \alpha-\beta \sum_{i=1}^n A_i=0
\end{array}
$$
}

\textcolor{blue}{
$$
\begin{aligned}
& \frac{\partial}{\partial \beta} \frac{1}{2 n} \sum_{i=1}\left(Y_i-\alpha-\beta A_i\right)^2=0 \\
& \Rightarrow \quad \frac{1}{2 n} \sum_{i=1}^n 2\left(Y_i-\alpha-\beta A_i\right)\left(-A_i\right)=0 \\
& \Rightarrow \quad \sum_{i=1}^n Y_i A_i-\alpha A_i-\beta A_i^2=0 \\
& \left\{\begin{array}{l}
\sum_{i=1}^n Y_i-n \alpha-\beta \sum_{i=1}^n A_i=0 \\
\sum_{i=1}^n Y_i A_i-\alpha \Sigma A_i-\beta \Sigma A_i^2= 0
\end{array}\right. \\
&
\end{aligned}
$$
}

\textcolor{blue}{
$$
\begin{gathered}
\hat{\beta}=\frac{\sum Y_i A_i-n \bar{A} \bar{Y}}{\sum A_i^2-n(\bar{A})^2} \\
\hat{\alpha}=\frac{1}{n} \cdot \sum Y_{i-} \frac{\sum Y_i A_i-n \bar{A} \bar{Y}}{\sum A_i^2-n \bar{A}^2} \cdot \sum A_i
\end{gathered}
$$
where $\bar{A}=\frac{1}{n} \sum_{i=1}^n A_i \quad \bar{Y}=\frac{1}{n} \sum_{i=1}^n Y_i$
}


### b)

\textcolor{blue}{In class and in Problem 2, we have proved that difference in means is valid estimater for the ATE, i.e., $\bar Y_1 - \bar Y_0$}

\textcolor{blue}{
$$
\begin{aligned}
& \bar{Y}_1-\bar{Y}_0 \\
= & \frac{1}{m} \sum_{i=1}^n A_i Y_i-\frac{1}{n-m} \sum_{i=1}^n\left(1-A_i\right) Y_i \\
= & \frac{(1-m) \sum_{i=1}^n A_i Y_i-m \sum_{i=1}^n\left(1-A_i\right) Y_i}{m(n-m)} \\
= & \frac{n \sum_{i=1}^n A_i Y_i-m \sum_{i=1}^n Y_i}{m(n-m)}
\end{aligned}
$$
}

\textcolor{blue}{
$$
\begin{aligned}
\hat \beta = \frac{\sum Y_i A_i-n \bar{A} \bar{Y}}{\sum A_i^2-n(\bar{A})^2} & =\frac{\sum_{i=1}^n Y_i A_i-\sum_{i=1}^n A_i \cdot \frac{1}{n} \sum_{i=1}^n Y_i}{\sum_{i=1}^n A_i^2-n\left(\frac{1}{n} \sum_{i=1}^n A_i\right)^2} \\
& =\frac{\sum_{i=1}^n Y_i A_i-m \cdot \frac{1}{n} \cdot \sum_{i=1}^n Y_i}{m-\frac{1}{n} \cdot m^2} \\
& =\frac{n \sum_{i=1}^n Y_i A_i-m \sum_{i=1}^n Y_i}{m n-m^2} \\
& =\bar{Y}_1-\bar{Y}_0
\end{aligned}
$$
}

\textcolor{blue}{Thus the OLS estimator $\hat \beta$ is just the difference in means. We have proved that the difference in means is a valid estimator for the ATE. So does $\hat \beta$.}

{{< pagebreak >}}

## References

::: {#refs}
:::

